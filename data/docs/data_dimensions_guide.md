# Data Dimensions Guide

## Document Information
- **Version**: Auto-generated
- **Updated**: 2026-01-03
- **Generated by**: dimension_checker.py
- **Purpose**: Complete guide to understanding data dimensions throughout the processing pipeline

---

## Current Configuration

### Processing Parameters
```yaml
# Signal Processing
bandpass_enabled: True
bandpass_lowcut: 8e6           # 8 MHz
bandpass_highcut: 12e6         # 12 MHz
bandpass_fs: 50e6              # 50 MSPS
bandpass_order: 5

tgc_enabled: True              # Time Gain Compensation
tgc_freq: 50e6                 # 50 MSPS
tgc_coef_att: 0.3              # Attenuation coefficient dB/(MHz·cm)

clip_enabled: True
clip_initial_size: 1996
clip_samples2remove_start: 107
clip_samples2remove_end: 589
# Resulting height: 1996 - 107 - 589 = 1300 samples

envelope_enabled: True
envelope_interp: False
envelope_padding_enabled: True
envelope_padding_mode: "constant"
envelope_padding_amount: 30

logcompression_enabled: False
logcompression_db: 20

normalization_enabled: True
normalization_method: "peakZ"  # Peak normalization → Z-normalization

decimation_enabled: True
decimation_factor: 10          # 50MSPS → 5MSPS

# Tokenization & Sequencing
token_window: 5
token_stride: 2
sequence_window: 10
startend_markers: False
```

---

## Signal Processing Pipeline Order

The processing is applied in the following sequence in `_signal_processing()`:

1. **Bandpass Filtering** - Butterworth filter (8-12 MHz)
2. **Time Gain Compensation (TGC)** - Depth-dependent gain
3. **Clipping** - Remove start/end samples
4. **Envelope Detection** - Analytic signal → envelope
5. **Log Compression** - Dynamic range compression (optional)
6. **Normalization** - peakZ: peak_normalization() → Z_normalization()
7. **Decimation** - Downsample by factor

---

## Data Flow & Dimension Transformations

### Processing Pipeline Dimensions

### 1. Raw Input
- **Shape**: `[channels, time_samples, a_mode_lines]` → `[3, ~65000, ~5000]`
- **Description**: Raw ultrasound data (3 channels)

### 2. After Bandpass + TGC
- **Shape**: `[3, ~65000, ~5000]` (unchanged)
- **Description**: Filtered and gain-compensated signal

### 3. After Clipping
- **Shape**: `[3, 1300, ~5000]`
- **Description**: Clipped: start=107, end=589 removed from initial_size=1996
- **Formula**: `height = initial_size - samples2remove_start - samples2remove_end`

### 4. After Envelope + Normalization
- **Shape**: `[3, 1300, ~5000]` (unchanged)
- **Description**: Envelope detected, peak & Z normalized

### 5. After Decimation
- **Shape**: `[3, 130, ~5000]`
- **Description**: Decimated by factor 10
- **Formula**: `height_decimated = height_clipped // decimation_factor`

### 6. After Tokenization
- **Shape**: `[num_tokens, 3, 130, 5]`
- **Description**: Sliding windows with window=5, stride=2
- **Formula**: `num_tokens = (a_mode_lines - token_window) // token_stride + 1`
- **Example**: For 5000 a_mode_lines: `(5000-5)//2 + 1 = 2498 tokens`
- **Token overlap**: `token_window - token_stride = 3 samples`

### 7. After Sequencing
- **Shape**: `[num_sequences, 10, 3, 130, 5]`
- **Description**: Grouped into sequences of 10 tokens
- **Formula**: `num_sequences = num_tokens // sequence_window`
- **Example**: `2498 // 10 = 249 sequences` (8 tokens clipped)
- **Tokens clipped**: `num_tokens % sequence_window`

### Optional: Start/End Markers
- If `startend_markers: True`: height dimension becomes `130 + 2 = 132`
- Adds fixed value markers at start/end of each A-mode line

---

## Memory Usage

- **Total elements per experiment**: `num_sequences × 10 × 3 × 130 × 5`
- **Example**: 249 × 10 × 3 × 130 × 5 = 4,855,500 elements
- **Memory per experiment**: ~18.5 MB (float32)
- **Memory formula**: `num_sequences × sequence_window × channels × height × token_window × 4 bytes`

---

## Output File Structure

### HDF5 File Format
Each experiment produces a file: `S{session}_P{participant}_E{experiment}_Xy.h5`

```
output_folder/
├── P0001/
│   ├── S0001_P0001_E0001_Xy.h5
│   ├── S0001_P0001_E0002_Xy.h5
│   └── ...
├── metadata.csv
├── replication_info.yaml
└── processing_log.yaml
```

### HDF5 Keys
- `X`: Sequence data - Shape: `[num_sequences, 10, 3, 130, 5]`
- `y`: Labels - Shape: `[num_sequences, 10, 1]`

---

## Dimension Calculation Formulas

```python
# Height after clipping
height_clipped = initial_size - samples2remove_start - samples2remove_end
# Example: 1996 - 107 - 589 = 1300

# Height after decimation
height_decimated = height_clipped // decimation_factor
# Example: 1300 // 10 = 130

# Height with start/end markers (optional)
height_with_markers = height_decimated + 2 if startend_markers else height_decimated

# Number of tokens per experiment
num_tokens = (a_mode_lines - token_window) // token_stride + 1

# Number of sequences per experiment
num_sequences = num_tokens // sequence_window

# Tokens clipped (don't fit into sequences)
tokens_clipped = num_tokens % sequence_window

# Final output shape
output_shape = [num_sequences, sequence_window, channels, height_final, token_window]
# Example: [249, 10, 3, 130, 5]
```

---

## Label Encoding

Labels are derived from joystick data (x, y positions):

```python
# Label encoding scheme (4 quadrants)
label = (x_label < 0).astype(int) * 2 + (y_label < 0).astype(int)
# Results in labels: 0, 1, 2, 3 (representing quadrants)
```

---

## Loading Processed Data

### Python Example
```python
import h5py
import numpy as np

# Load single experiment
with h5py.File('S0001_P0001_E0001_Xy.h5', 'r') as f:
    data = f['X'][:]    # Shape: [num_sequences, 10, 3, 130, 5]
    labels = f['y'][:]  # Shape: [num_sequences, 10, 1]

# For PyTorch
import torch
data_tensor = torch.from_numpy(data).float()
label_tensor = torch.from_numpy(labels).long()
```

### Expected Shapes
| Stage | Shape | Description |
|-------|-------|-------------|
| Single file load | `[num_seq, 10, 3, 130, 5]` | One experiment |
| Batched | `[batch, 10, 3, 130, 5]` | Model input |
| Token level | `[batch, 3, 130, 5]` | Single token |

---

## Metadata Structure

The `metadata.csv` contains:
- `token_id local`: Token ID within experiment
- `start`, `end`: Sample indices in original signal
- `participant`: Participant ID
- `session`: Session ID
- `experiment`: Experiment ID
- `token label`: Quadrant label (0-3)
- `sequence id`: Which sequence the token belongs to
- `file path`: Relative path to HDF5 file

---

## Validation Checklist

- [ ] Config `token_window` matches data width dimension (5)
- [ ] Config `sequence_window` matches second dimension (10)
- [ ] Height = `(initial_size - start - end) // decimation_factor`
- [ ] Height includes +2 if start/end markers enabled
- [ ] Number of sequences × sequence_window ≤ total tokens
- [ ] Metadata CSV row count matches expected tokens

---

## Common Issues and Solutions

### 1. Dimension Mismatch in Model
**Problem**: Model expects different input shape
**Solution**: Check if start/end markers are consistently enabled/disabled between preprocessing and training

### 2. Memory Errors During Training
**Problem**: Out of memory errors
**Solution**: Reduce batch size or sequence_window

### 3. Tokens Don't Fit Into Sequences
**Problem**: Many tokens are clipped
**Solution**: Adjust token_stride or sequence_window for better fit

### 4. Normalization File Mismatch
**Problem**: Error about normalization file suggesting wrong processing
**Solution**: Ensure `minmax_path` file matches envelope/logcompression settings

### 5. Signal Lag from Envelope Detection
**Note**: The advanced envelope padding options (interp, padding) were reverted to simple `envelope(analytic_signal(data))` due to signal lag issues.

---

## Reproducibility

Each processing run saves:
- `replication_info.yaml`: All parameters and paths
- `processing_log.yaml`: Stage-by-stage logging
- Copy of normalization minmax file (if used)

---

*This guide is automatically generated and updated by `dimension_checker.py`*
