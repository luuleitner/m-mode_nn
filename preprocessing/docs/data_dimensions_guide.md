# Data Dimensions Guide

## Document Information
- **Version**: Auto-generated
- **Updated**: 2026-01-03
- **Generated by**: dimension_checker.py
- **Purpose**: Complete guide to understanding data dimensions throughout the processing pipeline

---

## Current Configuration

### Processing Parameters
```yaml
# Signal Processing
bandpass_enabled: True
bandpass_lowcut: 8e6           # 8 MHz
bandpass_highcut: 12e6         # 12 MHz
bandpass_fs: 50e6              # 50 MSPS
bandpass_order: 5

tgc_enabled: True              # Time Gain Compensation
tgc_freq: 50e6                 # 50 MSPS
tgc_coef_att: 0.3              # Attenuation coefficient dB/(MHz·cm)

clip_enabled: True
clip_initial_size: 1996
clip_samples2remove_start: 107
clip_samples2remove_end: 589
# Resulting height: 1996 - 107 - 589 = 1300 samples

envelope_enabled: True
envelope_interp: False
envelope_padding_enabled: True
envelope_padding_mode: "constant"
envelope_padding_amount: 30

logcompression_enabled: False
logcompression_db: 20

normalization_enabled: True
normalization_method: "peakZ"  # Peak normalization → Z-normalization

decimation_enabled: True
decimation_factor: 10          # 50MSPS → 5MSPS

# Tokenization & Sequencing
token_window: 5
token_stride: 2
sequence_window: 10
startend_markers: False

# Output Mode (NEW)
output_mode: "transformer"   # transformer | flat
# transformer: Sequences for transformer training [num_seq, seq_win, C, H, W]
# flat: Direct tokens for CNN training [num_tokens, C, H, W]

# Labeling Pipeline (NEW)
label_method: "derivative"   # derivative | edge_to_peak | edge_to_derivative
label_threshold_percent: 5.0
label_axis: "x"              # x | y | combined

# Soft Labels (NEW)
soft_labels_enabled: false
soft_labels_num_classes: 3   # 0=noise, 1=up, 2=down
soft_labels_weighting: "gaussian"  # uniform | gaussian
soft_labels_gaussian_sigma_ratio: 0.25
```

---

## Dimension Terminology

| Term | Physical Meaning                 | Sampling | Axis in Code |
|------|----------------------------------|----------|--------------|
| **A-mode samples** | Fast-time (depth/axial)         | 50 MHz | axis 1 after stack |
| **Pulses** | Slow-time (M-mode temporal axis) | ~50 Hz PRF | axis 2 after stack |
| **Channels** | US transducer channels           | - | axis 0 after stack |

---

## Raw Data Format

### Ultrasound Files (`_US_ch{1,2,3}.npy`)
- **On-disk shape**: `[pulses, A-mode_samples]` → `[~10000, 1996]`
- **Axis 0**: Pulse repetitions (~10,000-11,500 per experiment, ~50 Hz PRF)
- **Axis 1**: A-mode samples per pulse (1996 samples @ 50 MHz)
- **dtype**: int16

### Joystick File (`_joystick.npy`)
- **On-disk shape**: `[samples, channels]` → `[~10000, 4]`
- **Axis 0**: Time samples (synchronized with US pulse count)
- **Axis 1**: Channels
  - `[:,0]` = unused
  - `[:,1]` = **X position**
  - `[:,2]` = **Y position**
  - `[:,3]` = **Trigger**
- **dtype**: int16 or float64

---

## Signal Processing Pipeline Order

The processing is applied in the following sequence in `_signal_processing()`:

1. **Bandpass Filtering** - Butterworth filter (8-12 MHz) on A-mode axis
2. **Time Gain Compensation (TGC)** - Depth-dependent gain on A-mode axis
3. **Clipping** - Remove start/end A-mode samples
4. **Envelope Detection** - Analytic signal → envelope
5. **Log Compression** - Dynamic range compression (optional)
6. **Normalization** - peakZ: peak_normalization() → Z_normalization()
7. **Decimation** - Downsample A-mode axis by factor

---

## Data Flow & Dimension Transformations

### Loading & Stacking (processor.py lines 397-407)

```
Raw File (per channel):    [pulses, A-mode] = [~10000, 1996]
                                    ↓ Transpose (d.T)
After Transpose:           [A-mode, pulses] = [1996, ~10000]
                                    ↓ np.stack(..., axis=0)
After Stack (3 channels):  [ch, A-mode, pulses] = [3, 1996, ~10000]
```

### Signal Processing Pipeline

### 1. Raw Input (after load & stack)
- **Shape**: `[channels, A-mode_samples, pulses]` → `[3, 1996, ~10000]`
- **Description**: 3 US channels, 1996 depth samples, ~10000 temporal pulses

### 2. After Bandpass + TGC
- **Shape**: `[3, 1996, ~10000]` (unchanged)
- **Description**: Filtered and gain-compensated signal

### 3. After Clipping
- **Shape**: `[3, 1300, ~10000]`
- **Description**: A-mode axis clipped: start=107, end=589 removed from 1996 samples
- **Formula**: `A-mode_clipped = initial_size - samples2remove_start - samples2remove_end`
- **Example**: `1996 - 107 - 589 = 1300`

### 4. After Envelope + Normalization
- **Shape**: `[3, 1300, ~10000]` (unchanged)
- **Description**: Envelope detected, peak & Z normalized

### 5. After Decimation
- **Shape**: `[3, 130, ~10000]`
- **Description**: A-mode axis decimated by factor 10
- **Formula**: `A-mode_decimated = A-mode_clipped // decimation_factor`
- **Example**: `1300 // 10 = 130`

### 6. After Tokenization (lines 493-506)
- **Input**: `[3, 130, ~10000]`
- **Operation**: Sliding windows along **pulse axis** (axis 2)
- **Output Shape**: `[num_tokens, 3, 130, 5]`
- **Formula**: `num_tokens = (pulses - token_window) // token_stride + 1`
- **Example**: For 10000 pulses: `(10000-5)//2 + 1 = 4998 tokens`
- **Token overlap**: `token_window - token_stride = 3 pulses`

### 7. After Sequencing/Output (depends on output_mode)

#### Transformer Mode (output_mode: "transformer")
- **Shape**: `[num_sequences, 10, 3, 130, 5]`
- **Description**: Tokens grouped into sequences of 10
- **Formula**: `num_sequences = num_tokens // sequence_window`
- **Example**: `4998 // 10 = 499 sequences` (8 tokens clipped)
- **Tokens clipped**: `num_tokens % sequence_window`
- **Label Shape**: `[num_sequences, 10, 1]` (hard labels)

#### Flat Mode (output_mode: "flat") - NEW for CNN
- **Shape**: `[num_tokens, 3, 130, 5]`
- **Description**: Direct token output, no sequencing
- **Example**: `4998 tokens` (no clipping)
- **Hard Label Shape**: `[num_tokens, 1]` with integer values (0, 1, 2)
- **Soft Label Shape**: `[num_tokens, 3]` with float probabilities summing to 1.0

### Dimension Summary Table

| Stage | Shape | Description |
|-------|-------|-------------|
| Raw file (per ch) | `[~10000, 1996]` | On-disk: [pulses, A-mode] |
| After transpose | `[1996, ~10000]` | [A-mode, pulses] |
| After stack | `[3, 1996, ~10000]` | [ch, A-mode, pulses] |
| After clip | `[3, 1300, ~10000]` | A-mode trimmed |
| After decimate | `[3, 130, ~10000]` | A-mode downsampled |
| After tokenize | `[~4998, 3, 130, 5]` | [tokens, ch, A-mode, token_win] |
| After sequence (transformer) | `[~499, 10, 3, 130, 5]` | [seq, seq_win, ch, A-mode, token_win] |
| After flat output (CNN) | `[~4998, 3, 130, 5]` | [tokens, ch, A-mode, token_win] |

### Optional: Start/End Markers
- If `startend_markers: True`: A-mode dimension becomes `130 + 2 = 132`
- Adds fixed value markers at start/end of each A-mode line

---

## Memory Usage

- **Total elements per experiment**: `num_sequences × seq_window × channels × A-mode × token_window`
- **Example**: 499 × 10 × 3 × 130 × 5 = 9,730,500 elements
- **Memory per experiment**: ~37 MB (float32)
- **Memory formula**: `num_sequences × 10 × 3 × 130 × 5 × 4 bytes`

---

## Output File Structure

### HDF5 File Format
Each experiment produces a file: `S{session}_P{participant}_E{experiment}_Xy.h5`

```
output_folder/
├── P0001/
│   ├── S0001_P0001_E0001_Xy.h5
│   ├── S0001_P0001_E0002_Xy.h5
│   └── ...
├── metadata.csv
├── replication_info.yaml
└── processing_log.yaml
```

### HDF5 Keys

**Transformer Mode:**
- `token`: Sequence data - Shape: `[num_sequences, 10, 3, 130, 5]`
- `label`: Labels - Shape: `[num_sequences, 10, 1]` (int64)

**Flat Mode (CNN):**
- `token`: Token data - Shape: `[num_tokens, 3, 130, 5]`
- `label`: Labels - Shape varies:
  - Hard labels: `[num_tokens, 1]` (int64)
  - Soft labels: `[num_tokens, num_classes]` (float32)

---

## Dimension Calculation Formulas

```python
# A-mode samples after clipping
A_mode_clipped = initial_size - samples2remove_start - samples2remove_end
# Example: 1996 - 107 - 589 = 1300

# A-mode samples after decimation
A_mode_decimated = A_mode_clipped // decimation_factor
# Example: 1300 // 10 = 130

# A-mode with start/end markers (optional)
A_mode_with_markers = A_mode_decimated + 2 if startend_markers else A_mode_decimated

# Number of tokens per experiment (sliding windows along PULSE axis)
num_tokens = (num_pulses - token_window) // token_stride + 1
# Example: (10000 - 5) // 2 + 1 = 4998

# Number of sequences per experiment
num_sequences = num_tokens // sequence_window
# Example: 4998 // 10 = 499

# Tokens clipped (don't fit into sequences)
tokens_clipped = num_tokens % sequence_window
# Example: 4998 % 10 = 8

# Final output shape
output_shape = [num_sequences, sequence_window, channels, A_mode_decimated, token_window]
# Example: [499, 10, 3, 130, 5]
```

---

## Label Encoding

### Labeling Pipeline (NEW)

Labels are created using the labeling pipeline from `preprocessing/label_logic/labeling.py`:

**Step 1: Hard Label Creation** (per-sample)
```python
# Methods available (configured via preprocess.labels.method):
# - "derivative": Threshold on derivative signal
# - "edge_to_peak": Position edge detection + derivative peak finding
# - "edge_to_derivative": Position edge + derivative threshold crossing

# Label values:
#   0 = noise (neutral)
#   1 = upward movement intention
#   2 = downward movement intention
```

**Step 2: Token-Level Aggregation**
```python
# For hard labels (soft_labels.enabled: false):
#   - Majority vote within each token window
#   - Output: [num_tokens, 1] int64

# For soft labels (soft_labels.enabled: true):
#   - Weighted probability distribution per class
#   - Weighting: "uniform" or "gaussian" (center-weighted)
#   - Output: [num_tokens, num_classes] float32, each row sums to 1.0
```

### Soft Label Example
```python
# Token window covers samples with labels: [0, 0, 1, 1, 1]
# With uniform weighting:
#   class 0 probability: 2/5 = 0.4
#   class 1 probability: 3/5 = 0.6
#   class 2 probability: 0/5 = 0.0
# Soft label: [0.4, 0.6, 0.0]
```

### Legacy Label Encoding (for reference)

Previous labeling used quadrant-based encoding:

```python
# Joystick channels: [:,1] = X, [:,2] = Y, [:,3] = trigger
# Label encoding scheme (4 quadrants)
label = (x_label < 0).astype(int) * 2 + (y_label < 0).astype(int)
# Results in labels: 0, 1, 2, 3 (representing quadrants)
```

---

## Loading Processed Data

### Python Example - Transformer Mode
```python
import h5py
import torch

# Load single experiment (transformer mode)
with h5py.File('S0001_P0001_E0001_Xy.h5', 'r') as f:
    data = f['token'][:]   # Shape: [num_sequences, 10, 3, 130, 5]
    labels = f['label'][:] # Shape: [num_sequences, 10, 1]

# For PyTorch
data_tensor = torch.from_numpy(data).float()
label_tensor = torch.from_numpy(labels).long()
```

### Python Example - Flat Mode (CNN)
```python
import h5py
import torch

# Load single experiment (flat mode with soft labels)
with h5py.File('S0001_P0001_E0001_Xy.h5', 'r') as f:
    data = f['token'][:]   # Shape: [num_tokens, 3, 130, 5]
    labels = f['label'][:] # Shape: [num_tokens, 3] (soft) or [num_tokens, 1] (hard)

# For PyTorch with soft labels
data_tensor = torch.from_numpy(data).float()
label_tensor = torch.from_numpy(labels).float()  # float for soft labels

# Verify soft labels sum to 1
assert torch.allclose(label_tensor.sum(dim=1), torch.ones(len(label_tensor)))
```

### Expected Shapes

**Transformer Mode:**
| Stage | Shape | Dimensions |
|-------|-------|------------|
| Single file load | `[~499, 10, 3, 130, 5]` | [seq, seq_win, ch, A-mode, token_win] |
| Batched | `[batch, 10, 3, 130, 5]` | Model input |
| Token level | `[batch, 3, 130, 5]` | Single token: [ch, A-mode, pulses] |

**Flat Mode (CNN):**
| Stage | Shape | Dimensions |
|-------|-------|------------|
| Single file load | `[~4998, 3, 130, 5]` | [tokens, ch, A-mode, token_win] |
| Batched | `[batch, 3, 130, 5]` | CNN input |
| Soft labels | `[batch, 3]` | Probability distribution |
| Hard labels | `[batch, 1]` | Class index |

---

## Metadata Structure

The `metadata.csv` contains:
- `token_id local`: Token ID within experiment
- `start`, `end`: Pulse indices in original signal (along slow-time axis)
- `participant`: Participant ID
- `session`: Session ID
- `experiment`: Experiment ID
- `token label`: Quadrant label (0-3)
- `sequence id`: Which sequence the token belongs to
- `file path`: Relative path to HDF5 file

---

## Validation Checklist

- [ ] Config `token_window` matches last dimension (5 pulses per token)
- [ ] Config `sequence_window` matches second dimension (10 tokens per sequence)
- [ ] A-mode = `(initial_size - start - end) // decimation_factor` = 130
- [ ] A-mode includes +2 if start/end markers enabled
- [ ] Number of sequences × sequence_window ≤ total tokens
- [ ] Metadata CSV row count matches expected tokens

---

## Common Issues and Solutions

### 1. Dimension Mismatch in Model
**Problem**: Model expects different input shape
**Solution**: Check if start/end markers are consistently enabled/disabled between preprocessing and training

### 2. Memory Errors During Training
**Problem**: Out of memory errors
**Solution**: Reduce batch size or sequence_window

### 3. Tokens Don't Fit Into Sequences
**Problem**: Many tokens are clipped
**Solution**: Adjust token_stride or sequence_window for better fit

### 4. Normalization File Mismatch
**Problem**: Error about normalization file suggesting wrong processing
**Solution**: Ensure `minmax_path` file matches envelope/logcompression settings

### 5. Signal Lag from Envelope Detection
**Note**: The advanced envelope padding options (interp, padding) were reverted to simple `envelope(analytic_signal(data))` due to signal lag issues.

---

## Reproducibility

Each processing run saves:
- `replication_info.yaml`: All parameters and paths
- `processing_log.yaml`: Stage-by-stage logging
- Copy of normalization minmax file (if used)

---

*This guide is automatically generated and updated by `dimension_checker.py`*
