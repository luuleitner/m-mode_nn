# ========================================
# COMPREHENSIVE TRAINING CONFIGURATION
# ========================================

# Global Settings
global_setting:
  run:
    type: embedding                   # embedding, classifier
    mode: run                         # debug, run, overfit
    behaviour: deterministic          # deterministic, probabilistic
    device: cuda                      # cpu, cuda

    config:
      debug:
        level: 2                      # 1, 2, 3
      deterministic:
        seed: &global_seed 33

  paths:
    base_data_path: &base_data_path "/vol/data/2026_wristus_wiicontroller_sgambato/day002"
    process_base_data_path: &process_base_data_path "/vol/data/2026_wristus_wiicontroller_sgambato/day002/processed"
    train_base_data_path: &train_base_data_path "/home/cleitner/data/2026_wristus_wiicontroller_sgambato/day002/processed/Dataset_Envelope_CNN/Env_Dec10_Log50_Npeak_D1_Pclip99_W18_S04_Lsoft/latest"


# ========================================
# PREPROCESSING CONFIGURATION
# ========================================
preprocess:
  data:
    basepath: *base_data_path
    id: "Dataset_Envelope_CNN"
    strategy: "all"                   # all | selection_file
    selection_file: null              # Used when strategy=selection_file (run utils/generate_selection.py)
    save_ftype: "h5"                  # h5, zarr

  signal:
    clip:
      apply: true
      initial_size: 1996
      samples2remove_start: 107
      samples2remove_end: 589
    tgc:
      apply: true
      freq: 50e6                      # 50 MSPS
      coef_att: 0.3                   # Attenuation coefficient in dB/(MHz·cm)
    bandpass:
      apply: true
      lowcut: 8e6                     # 8 MHz
      highcut: 12e6                   # 12 MHz
      fs: 50e6                        # 50 MSPS
      order: 10
    envelope:
      apply: true
      interp: false
      padding:
        apply: true
        mode: "constant"
        amount: 30
      lowpass:
        apply: true
        mode: "auto"                    # auto | manual
        manual_cutoff: 2e6              # Used only when mode="manual"
        order: 4
    decimation:
      apply: true
      factor: 10                      # 50MSPS -> 5MSPS
    logcompression:
      apply: true
      db: 50
    differentiation:
      apply: false                   # Temporal differentiation (gradient along pulse/slow-time axis)
      method: "gradient"             # gradient | diff
      order: 1                       # 1 = velocity, 2 = acceleration
    percentile_clip:
      apply: false                  # Clip extreme values to reduce outlier impact
      percentile: 99
      symmetric: true                # true: clip to [-p, +p] using abs values
    normalization:                    # Always last — ensures network sees a known, consistent range
      apply: true
      method: "peak"

  tokenization:
    tokens2file: true
    startendID: false
    window: 25                        # Pulses per token
    stride: 5                         # Pulse stride between tokens

  output:
    mode: "flat"                      # flat: tokens saved individually, sequencing at dataset level

  label_config: "preprocessing/label_logic/label_config.yaml"


# ========================================
# ML PIPELINE CONFIGURATION
# ========================================
ml:
  # =====================================================================
  # DATASET CONFIGURATION
  # =====================================================================
  #  H5 files → Global filters → Test/Val selection → Split → Train remainder
  #
  loading:
    load_data_pickle: false

  dataset:
    data_root: *train_base_data_path
    target_batch_size: 250
    dataset_key: 'token'
    random_seed: *global_seed

    # --- STEP 1: GLOBAL FILTERS ---
    # Applied to ALL data BEFORE splitting. Filters combine with AND logic.
    global_participant_filter: null         # e.g., [1, 2, 3]
    global_session_filter: null             # e.g., [14, 15]
    global_experiment_filter: null          # e.g., [0, 1, 2]
    global_label_filter: null               # e.g., [1, 2, 3, 4] (auto-set when include_noise=false)

    # --- STEP 2: TEST/VAL SELECTION ---
    test_val_strategy: "random"             # "filter" or "random"

    # Option A: FILTER strategy (AND logic)
    test_val_participant_filter: null
    test_val_session_filter: null
    test_val_experiment_filter: null
    test_val_label_filter: null

    # Option B: RANDOM strategy
    test_val_random_experiments: 0.2        # Count (>=1) or percentage (0-1)
    test_val_multi_session: true            # Prefer experiments from different sessions

    # --- STEP 3: TEST/VAL SPLIT ---
    test_val_split_ratio: 0.5               # 0.5 = 50% test, 50% val
    split_level: "experiment"               # "sequence" or "experiment"

    # --- STEP 4: SHUFFLING ---
    shuffle_experiments: true
    shuffle_sequences: true

    # --- STEP 5: SEQUENCE GROUPING (train only) ---
    # Groups consecutive tokens into overlapping sequences using a sliding window.
    # Shuffling operates at the sequence level, preserving temporal order within.
    #
    #   Tokens:  [T0, T1, T2, T3, T4, T5, T6, T7, ...]
    #   seq_len=4, seq_stride=2:
    #     S0: [T0, T1, T2, T3]
    #     S1: [T2, T3, T4, T5]   (overlap = seq_len - seq_stride = 2)
    #     S2: [T4, T5, T6, T7]
    #   Shuffle: [S2, S0, S1, ...] — sequences shuffled, tokens ordered within
    #
    sequence_grouping:
      enabled: true
      seq_len: 14                       # Tokens per sequence
      seq_stride: 5                     # Stride between sequences (overlap = seq_len - seq_stride)

    # --- STEP 6: CLASS BALANCING (train only) ---
    balance_classes: false                  # Using weighted loss instead
    balance_strategy: "oversample"

    oversample_config:
      method: "augment"                     # duplicate | augment | mixed
      target_ratio: 0.2
      augment_ratio: 0.3                    # For 'mixed': ratio of augmented vs duplicated
      augmentations:
        noise:
          enabled: true
          std: 0.02
        scale:
          enabled: true
          range: [0.9, 1.1]
        shift:
          enabled: true
          max_shift: 3

    # --- STEP 7: ON-THE-FLY AUGMENTATION (train only) ---
    # Stochastic augmentation applied to ALL training samples every epoch.
    train_augmentation:
      enabled: true
      noise:
        enabled: true
        std: 0.05
      scale:
        enabled: true
        range: [0.9, 1.1]
      shift:
        enabled: true
        max_shift: 3

  # =====================================================================
  # AUTOENCODER MODEL CONFIGURATION
  # =====================================================================
  model:
    type: "UNetAutoencoder"           # CNNAutoencoder | UNetAutoencoder
    channels_per_layer: [32, 64, 128]
    embedding_dim: 256
    skip_drop_prob: 0.5              # Probability of dropping ALL skip connections during training (0=full UNet, 1=vanilla AE)

  # =====================================================================
  # XGB CLASSIFIER (embedding-based)
  # =====================================================================
  classifier:
    type: "XGBoost"
    xgboost:
      n_estimators: 300
      max_depth: 6
      learning_rate: 0.05
      subsample: 0.8
      colsample_bytree: 0.8
      reg_alpha: 0.1
      reg_lambda: 1.0
      min_child_weight: 3
      gamma: 0.1
      early_stopping_rounds: 30

  # =====================================================================
  # CNN CLASSIFIER (DirectCNNClassifier)
  # =====================================================================
  cnn:
    # Width multiplier: scales channel counts
    #   1 = 16->32->64   (~48K params)
    #   2 = 32->64->128  (~187K params)
    #   4 = 64->128->256  (~740K params)
    width_multiplier: 2

  # =====================================================================
  # TRAINING CONFIGURATION
  # =====================================================================
  training:
    epochs: 200
    lr: 0.0002
    weight_decay: 0.03

    optimizer:
      type: "adamw"                   # adamw, adam, sgd
      betas: [0.9, 0.999]
      eps: 1e-8
      momentum: 0.9                   # For SGD

    lr_scheduler:
      type: "cosine_warmup"           # plateau, onecycle, cosine, cosine_warmup, step, none
      warmup_epochs: 5
      eta_min: 1e-6

    loss_weights:
      mse_weight: 0.2
      l1_weight: 0.2
      embedding_reg: 0.0001
      classification_weight: 0.8
      contrastive_weight: 0.1
      contrastive_temperature: 0.1

    regularization:
      grad_clip_norm: 1.0
      batch_norm: true
      dropout:
        spatial: 0.3
        fc: 0.5

    imbalance:
      class_weights:
        enabled: true
        method: "inverse_frequency"     # inverse_frequency | effective_samples | sqrt_inverse | custom
        power: 0.6                      # <1 gentle, =1 standard, >1 aggressive
        custom_weights: null            # e.g., [0.1, 1.0, 1.0, 1.0, 1.0]

    checkpointing:
      save_best: true
      save_every_n_epochs: 5
      checkpoint_path: *train_base_data_path

    restart:
      enable: true
      save_restart_every: 5

    validation:
      plot_every_n_epochs: 2

    early_stopping:
      enabled: true
      patience: 15
      min_delta: 0.0005
      monitor: "val_balanced_accuracy"  # val_loss | val_accuracy | val_balanced_accuracy


# ========================================
# CROSS-VALIDATION CONFIGURATION
# ========================================
# Use: python -m src.data.precompute_kfolds --config config/config.yaml
#
cross_validation:
  output_subdir: "cv_folds_test_P0"
  strategy: "participant_within"        # experiment_kfold | session_loso | participant_lopo | participant_within

  experiment_kfold:
    participant_filter: null
    session_filter: null
    n_folds: 5
    test_val_split_ratio: 0.5

  session_loso:
    participant_filter: null
    session_filter: null
    test_val_split_ratio: 0.5

  participant_lopo:
    participant_filter: null
    test_val_split_ratio: 0.5

  participant_within:
    participant_filter: null
    inner_folds: null                   # K-fold within each participant (null = single split)
    test_val_split_ratio: 0.5


# ========================================
# LOGGING
# ========================================
wandb:
  use_wandb: true
  project: "IEEE SAS 2026"
  api_key: "c1842818a3c8db66c3f75d77b4640749e64cdc0d"    # TODO: move to env variable
  name: "embedding_autoencoder"
  settings:
    watch_model: true


# ========================================
# EXPERIMENT METADATA
# ========================================
experiment:
  name: "ultrasound_embedding_v1"
  description: "1DCNN autoencoder for ultrasound M-mode dimensionality reduction"
  version: "1.0.0"

  resources:
    num_workers: 8
    pin_memory: true
    prefetch_factor: 4
