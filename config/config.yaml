# ========================================
# COMPREHENSIVE TRAINING CONFIGURATION
# ========================================

# Global Settings
global_setting:
  run:
    type: embedding                   # embedding, classifier
    mode: run                         # debug, run, overfit
    behaviour: deterministic          # deterministic, probabilistic
    device: cuda                      # cpu, cuda

    config:
      debug:
        level: 2                      # 1, 2, 3
      deterministic:
        seed: &global_seed 353

  paths:
    base_data_path: &base_data_path "/vol/data/2026_wristus_wiicontroller_sgambato/day002"
    process_base_data_path: &process_base_data_path "/vol/data/2026_wristus_wiicontroller_sgambato/day002/processed"
    train_base_data_path: &train_base_data_path "/vol/data/2026_wristus_wiicontroller_sgambato/day002/processed/Dataset_Envelope_CNN/Window10_Stride05_Labels_soft/run_20260130_135933"


# ========================================
# PREPROCESSING CONFIGURATION
# ========================================
preprocess:
  data:
    basepath: *base_data_path
    id: "Dataset_Envelope_CNN"
    strategy: "all"                   # all | selection_file
    selection_file: "/vol/data/2026_wristus_wiicontroller_sgambato/day002/raw/selection_20260129_202647.csv"   # Used when strategy=selection_file (run utils/generate_selection.py)
    save_ftype: "h5"                  # h5, zarr

  signal:
    clip:
      apply: true
      initial_size: 1996
      samples2remove_start: 107
      samples2remove_end: 589
    tgc:
      apply: true
      freq: 50e6                      # 50 MSPS
      coef_att: 0.3                   # Attenuation coefficient in dB/(MHz·cm) - 0.5 is the standard for soft tissue, adjust as needed
    bandpass:
      apply: true
      lowcut: 8e6                     # 8 MHz
      highcut: 12e6                   # 12 MHz
      fs: 50e6                        # 50 MSPS
      order: 10
    envelope:
      apply: true
      interp: false
      padding:
        apply: true
        mode: "constant"
        amount: 30
    logcompression:
      apply: true
      db: 45
    normalization:
      apply: true
      method: "peakZ"
      minmax_path: "/home/cleitner/code/lab/projects/ML/m-mode_nn/config/minmax_Envdata.csv"
    decimation:
      apply: false
      factor: 10                      # 50MSPS -> 5MSPS

  tokenization:
    tokens2file: true
    startendID: false
    window: 10
    stride: 5
    description: "Specifies the tokenization process."

  sequencing:
    window: 15

  output:
    mode: "flat"             # transformer | flat (flat for CNN, no sequencing)

  # Label configuration is in a separate file for clarity
  label_config: "preprocessing/label_logic/label_config.yaml"


# ========================================
# ML PIPELINE CONFIGURATION
# ========================================
ml:
  # =====================================================================
  # DATASET CONFIGURATION - DATA SAMPLING PIPELINE
  # =====================================================================
  # Data Loading
  loading:
    load_data_pickle: false
  #
  #  ┌─────────────────────────────────────────────────────────────────┐
  #  │  H5 FILES (from preprocessing)                                 │
  #  └──────────────────────────┬──────────────────────────────────────┘
  #                             │
  #                             ▼
  #  ┌─────────────────────────────────────────────────────────────────┐
  #  │  STEP 1: GLOBAL FILTERS (reduce dataset before splitting)      │
  #  │          Applied with AND logic to ALL data                    │
  #  └──────────────────────────┬──────────────────────────────────────┘
  #                             │
  #                             ▼
  #  ┌─────────────────────────────────────────────────────────────────┐
  #  │  STEP 2: TEST/VAL SELECTION (strategy: "filter" or "random")   │
  #  │          Select which data goes to test/val                    │
  #  └──────────────────────────┬──────────────────────────────────────┘
  #                             │
  #                             ▼
  #  ┌─────────────────────────────────────────────────────────────────┐
  #  │  STEP 3: SPLIT TEST/VAL (ratio + level)                        │
  #  │          Divide selected data into test and val sets           │
  #  └──────────────────────────┬──────────────────────────────────────┘
  #                             │
  #                             ▼
  #  ┌─────────────────────────────────────────────────────────────────┐
  #  │  STEP 4: TRAIN = REMAINDER (+ shuffling)                       │
  #  │          Everything not in test/val becomes training           │
  #  └─────────────────────────────────────────────────────────────────┘
  #
  dataset:
    # --- Data Source ---
    data_root: *train_base_data_path
    target_batch_size: &target_batch_size 50
    dataset_key: 'token'
    random_seed: *global_seed

    # =========================================
    # STEP 1: GLOBAL FILTERS
    # =========================================
    # Applied to ALL data BEFORE any splitting.
    # Use to reduce dataset (e.g., exclude bad sessions).
    # Filters combine with AND logic.
    global_participant_filter: null         # e.g., [1, 2, 3] - keep only these
    global_session_filter: null             # e.g., [14, 15] - keep only these
    global_experiment_filter: null          # e.g., [0, 1, 2] - keep only these
    global_label_filter: null               # e.g., [1, 2] - keep only movement

    # =========================================
    # STEP 2: TEST/VAL SELECTION STRATEGY
    # =========================================
    # Choose ONE strategy: "filter" or "random"
    test_val_strategy: "random"

    # --- Option A: FILTER strategy ---
    # Select test/val by matching criteria (AND logic)
    test_val_participant_filter: null       # e.g., [1, 2]
    test_val_session_filter: null           # e.g., [14] - session 14 for test/val
    test_val_experiment_filter: null        # e.g., [5, 6, 7]
    test_val_label_filter: null             # e.g., [1, 2]

    # --- Option B: RANDOM strategy ---
    # Randomly select experiments for test/val
    test_val_random_experiments: 0.2        # Count (>=1) or percentage (0-1), e.g., 3 or 0.1 for 10%
    test_val_multi_session: true            # Prefer experiments from different sessions

    # =========================================
    # STEP 3: TEST/VAL SPLIT
    # =========================================
    test_val_split_ratio: 0.5               # 0.5 = 50% test, 50% val
    split_level: "experiment"               # "sequence" or "experiment"

    # =========================================
    # STEP 4: SHUFFLING (for training)
    # =========================================
    shuffle_experiments: true               # Randomize experiment order
    shuffle_sequences: true                 # Randomize sequence order within experiments

    # =========================================
    # STEP 5: CLASS BALANCING (train set only)
    # =========================================
    # Two options: dataset-level (oversampling) OR loss-level (weighted loss)
    # For joint AE+classification: use weighted loss to preserve real 90/5/5 distribution
    balance_classes: true                  # Disabled: using weighted loss instead
    balance_strategy: "oversample"          # (unused when balance_classes=false)

    oversample_config:
      method: "mixed"                     # duplicate | augment | mixed
      target_ratio: 0.4                     # 1.0 = match majority class count
      augment_ratio: 0.4                    # For 'mixed': ratio of augmented vs duplicated

      # Augmentation settings (semantically preserving transforms)
      augmentations:
        noise:
          enabled: true
          std: 0.02                         # Gaussian noise standard deviation
        scale:
          enabled: true
          range: [0.9, 1.1]                 # Amplitude scaling range
        shift:
          enabled: true
          max_shift: 3                      # Max temporal shift in samples

  # Model Architecture
  model:
    type: "UNetAutoencoder"           # "CNNAutoencoder", "UNetAutoencoder", "TransformerAutoencoder"
    channels_per_layer: [32, 64, 128]       # 3 levels (preserves temporal dim=2 at bottleneck)
    embedding_dim: 512                # Less aggressive compression
    num_heads: 8                   # For transformer models
    num_layers: 4                  # For transformer models

  # =====================================================================
  # CLASSIFIER CONFIGURATION (for embedding-based classification)
  # =====================================================================
  classifier:
    type: "XGBoost"
    class_names: ["noise", "up", "down", "left", "right"]

    # XGBoost hyperparameters
    xgboost:
      n_estimators: 300              # More trees, early stopping finds optimum
      max_depth: 6
      learning_rate: 0.05            # Lower LR + more trees = better generalization
      subsample: 0.8
      colsample_bytree: 0.8
      reg_alpha: 0.1                 # Light L1 regularization
      reg_lambda: 1.0
      min_child_weight: 3            # Prevents overfitting on small leaves
      gamma: 0.1                     # Minimum loss reduction for split
      early_stopping_rounds: 30      # More patience for convergence

    # Embedding preprocessing
    preprocessing:
      normalize_embeddings: true
      scaler: "standard"              # standard | minmax | none

    # Hyperparameter tuning (optional)
    hyperparameter_search:
      enabled: false
      method: "optuna"                # optuna | grid | random
      n_trials: 50

  # =====================================================================
  # TRAINING CONFIGURATION
  # =====================================================================
  training:
    # Basic Training Parameters
    epochs: 50
    lr: 0.0001                        # Lower LR to reduce overfitting (was 0.0003)
    weight_decay: 0.02                # Stronger weight decay for regularization (was 0.01)

    # Optimizer Configuration
    optimizer:
      type: "adamw"                   # adamw, adam, sgd
      betas: [0.9, 0.999]            # For Adam/AdamW
      eps: 1e-8                      # For Adam/AdamW
      momentum: 0.9                  # For SGD

    # Learning Rate Scheduler
    lr_scheduler:
      type: "cosine"                  # plateau, onecycle, cosine, step, none

    # Loss Configuration
    loss_weights:
      mse_weight: 0.3
      l1_weight: 0.3                  # More L1 = sharper reconstructions
      embedding_reg: 0.0005
      classification_weight: 0.4     # Joint classification loss (balanced with reconstruction)

    # Training Regularization
    regularization:
      grad_clip_norm: 1.0            # Gradient clipping norm
      batch_norm: true               # Use batch normalization
      dropout:
        spatial: 0.1                 # Dropout2d between conv blocks (0.1-0.2 recommended)
        fc: 0.5                      # Dropout in FC classifier head (0.3-0.5 recommended)

    # Class Imbalance Handling
    # Use ONE or NONE of these - combining both is usually too aggressive
    imbalance:
      class_weights:
        enabled: true               # Inverse frequency weighting (--no-weights to disable)
      focal_loss:
        enabled: false                # Down-weight easy examples (--no-focal to disable)
        gamma: 1.5                   # Focusing parameter (higher = more focus on hard examples)

    # Checkpointing
    checkpointing:
      save_best: true
      save_every_n_epochs: 10        # Save regular checkpoint every N epochs
      checkpoint_path: *train_base_data_path

    # Restart Configuration
    restart:
      enable: true
      save_restart_every: 10         # Save restart checkpoint every N epochs

    # Validation and Evaluation
    validation:
      plot_every_n_epochs: 2         # Generate plots every N epochs

    # Early Stopping
    early_stopping:
      enabled: true                  # Enable early stopping
      patience: 15                   # Epochs to wait for improvement (was 25)
      min_delta: 0.001               # Minimum change to count as improvement (0.1% for balanced_accuracy)
      monitor: "val_balanced_accuracy"   # Metric to monitor: val_loss, val_accuracy, val_balanced_accuracy


# ========================================
# CROSS-VALIDATION CONFIGURATION
# ========================================
# For K-fold cross-validation experiments.
# Use: python -m src.data.precompute_kfolds --config config/config.yaml
#
# This creates multiple fold directories, each with train_ds.pkl, val_ds.pkl, test_ds.pkl
# The regular precompute_datasets.py creates a single split (non-CV mode).
#
cross_validation:
  output_subdir: "cv_folds"             # Folds saved to: {data_root}/{output_subdir}/fold_X/

  # =========================================
  # CV STRATEGY (choose one)
  # =========================================
  # Options:
  #   - "experiment_kfold": K-fold over experiments within selected participant(s)
  #   - "session_loso":     Leave-One-Session-Out
  #   - "participant_lopo": Leave-One-Participant-Out
  strategy: "experiment_kfold"

  # =========================================
  # STRATEGY 1: experiment_kfold
  # =========================================
  # K-fold cross-validation over experiments within specific participant(s).
  # Use case: Evaluate model on same participant(s) with different experiment splits.
  #
  # Example: participant_filter=[1], n_folds=5
  #   Participant 1 has 10 experiments → each fold holds out 2 experiments
  #
  experiment_kfold:
    participant_filter: null            # Which participant(s) to include [1,2] or null=all
    session_filter: null                # Optionally filter sessions too
    n_folds: 5                          # Number of folds
    test_val_split_ratio: 0.5           # Within holdout: 50% test, 50% val

  # =========================================
  # STRATEGY 2: session_loso
  # =========================================
  # Leave-One-Session-Out cross-validation.
  # Use case: Evaluate generalization across recording sessions.
  #
  # Example: 4 sessions → 4 folds, each holds out one session
  #
  session_loso:
    participant_filter: null            # Which participant(s) to include (null=all)
    session_filter: null                # Which sessions to fold over (null=all available)
    test_val_split_ratio: 0.5           # Within holdout session: 50% test, 50% val

  # =========================================
  # STRATEGY 3: participant_lopo
  # =========================================
  # Leave-One-Participant-Out cross-validation.
  # Use case: Evaluate generalization to new participants.
  #
  # Example: 3 participants → 3 folds, each holds out one participant
  #
  participant_lopo:
    participant_filter: null            # Which participants to fold over (null=all)
    test_val_split_ratio: 0.5           # Within holdout participant: 50% test, 50% val


# ========================================
# LOGGING CONFIGURATION
# ========================================
wandb:
  use_wandb: true
  project: "IEEE SAS 2026"
  api_key: "c1842818a3c8db66c3f75d77b4640749e64cdc0d"
  name: "embedding_autoencoder"
  notes: "1DCNN for dimensionality reduction with comprehensive restart support"
  tags:
    - "embedding"
    - "autoencoder"
    - "ultrasound"
    - "restart_enabled"

  # Advanced WandB settings
  settings:
    save_code: true
    log_gradients: false             # Log gradient histograms
    log_parameters: false            # Log parameter histograms
    watch_model: true                # Watch model with wandb.watch()

# ========================================
# EXPERIMENTAL CONFIGURATION
# ========================================
experiment:
  name: "ultrasound_embedding_v1"
  description: "1DCNN autoencoder for ultrasound M-mode dimensionality reduction"
  version: "1.0.0"

  # Reproducibility
  reproducibility:
    deterministic_algorithms: true    # Use deterministic algorithms when possible
    benchmark_mode: false            # CUDNN benchmark mode

  # Resource Management
  resources:
    num_workers: 8                   # DataLoader workers
    pin_memory: true                 # Pin memory for faster GPU transfer
    prefetch_factor: 4               # DataLoader prefetch factor

# ========================================
# ADVANCED TRAINING OPTIONS
# ========================================
advanced:
  # Mixed Precision Training
  mixed_precision:
    enable: false                    # Enable AMP (Automatic Mixed Precision)
    loss_scale: "dynamic"            # dynamic or float value

  # Distributed Training
  distributed:
    enable: false                    # Enable distributed training
    backend: "nccl"                  # nccl, gloo
    world_size: 1                    # Number of processes

  # Profiling
  profiling:
    enable: false                    # Enable PyTorch profiler
    trace_file: "trace.json"         # Profiler output file