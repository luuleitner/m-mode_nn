# CNNAutoencoder Architecture Analysis

## Document Information

| **Field** | **Value** |
|-----------|-----------|
| **Version** | 1.0 |
| **Created** | 2025-10-08 |
| **Model Class** | `CNNAutoencoder` |
| **File** | `models/cnnAE.py` |
| **Input Format** | `[B, T, C, H, W]` - Ultrasound M-mode sequences |
| **Purpose** | Dimensionality reduction and feature extraction |

---

## Layer Size Computation Formulas

### Convolution Layer Output Size
```
Output_Height = floor((Input_Height + 2√óPadding_H - Kernel_H) / Stride_H) + 1
Output_Width  = floor((Input_Width  + 2√óPadding_W - Kernel_W) / Stride_W) + 1
```

### Transposed Convolution (ConvTranspose2d) Output Size
```
Output_Height = (Input_Height - 1) √ó Stride_H - 2√óPadding_H + Kernel_H + Output_Padding_H
Output_Width  = (Input_Width  - 1) √ó Stride_W - 2√óPadding_W + Kernel_W + Output_Padding_W
```

### AdaptiveAvgPool2d Output Size
```
Output_Height = Target_Height (specified in parameters)
Output_Width  = Target_Width  (specified in parameters)
```

### Parameter Count Formulas
```
Conv2d Parameters    = (Kernel_H √ó Kernel_W √ó Input_Channels √ó Output_Channels) + Output_Channels
Linear Parameters    = (Input_Features √ó Output_Features) + Output_Features
BatchNorm2d Params   = 2 √ó Num_Channels  (Œ≥ and Œ≤ parameters)
```

### Memory Usage Estimation
```
Memory (bytes) = Batch_Size √ó Channels √ó Height √ó Width √ó 4  (for float32)
Memory (MB)    = Memory (bytes) / (1024 √ó 1024)
```

---

## Architecture Overview

The CNNAutoencoder is a specialized 1D CNN designed for ultrasound M-mode signal processing with width reduction capabilities. It compresses temporal ultrasound sequences into compact embeddings while preserving essential features for reconstruction.

### Key Design Principles
- **Width-First Reduction**: Reduces spatial width (5‚Üí1) before depth processing
- **Temporal Processing**: Handles sequences of ultrasound frames
- **Progressive Compression**: Gradual dimension reduction through encoder
- **Symmetric Reconstruction**: Mirror decoder architecture for reconstruction

---

## ASCII Architecture Diagram

```
INPUT: [B, T, 3, 132, 5]  ‚Üê‚îÄ‚îÄ Batch √ó Tokens √ó Channels √ó Height √ó Width
          ‚îÇ
          ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  WIDTH REDUCER  ‚îÇ  Conv2d(3‚Üí32, k=(1,5)) + BN + ReLU
   ‚îÇ   [B*T,32,132,1]‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   ENCODER       ‚îÇ
   ‚îÇ                 ‚îÇ
   ‚îÇ Conv1: 32‚Üí64    ‚îÇ  k=(3,1), s=(3,1) ‚Üí [B*T,64,44,1]
   ‚îÇ + BN + ReLU     ‚îÇ  + Dropout2d(0.1)
   ‚îÇ                 ‚îÇ
   ‚îÇ Conv2: 64‚Üí128   ‚îÇ  k=(3,1), s=(2,1) ‚Üí [B*T,128,22,1]  
   ‚îÇ + BN + ReLU     ‚îÇ  + Dropout2d(0.1)
   ‚îÇ                 ‚îÇ
   ‚îÇ Conv3: 128‚Üí256  ‚îÇ  k=(3,1), s=(2,1) ‚Üí [B*T,256,11,1]
   ‚îÇ + BN + ReLU     ‚îÇ
   ‚îÇ                 ‚îÇ
   ‚îÇ Conv4: 256‚Üí512  ‚îÇ  k=(3,1), s=(2,1) ‚Üí [B*T,512,5,1]
   ‚îÇ + BN + ReLU     ‚îÇ
   ‚îÇ                 ‚îÇ
   ‚îÇ AdaptiveAvgPool ‚îÇ  ‚Üí [B*T,512,1,1]
   ‚îÇ Flatten         ‚îÇ  ‚Üí [B*T,512]
   ‚îÇ Linear(512‚ÜíEMB) ‚îÇ  ‚Üí [B*T,embedding_dim]
   ‚îÇ LayerNorm       ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ EMBEDDING   ‚îÇ  [B, T, embedding_dim]
      ‚îÇ  BOTTLENECK ‚îÇ  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   DECODER       ‚îÇ
   ‚îÇ                 ‚îÇ
   ‚îÇ Linear Proj     ‚îÇ  embedding_dim ‚Üí 512*5
   ‚îÇ + ReLU          ‚îÇ  ‚Üí [B*T,2560] ‚Üí [B*T,512,5,1]
   ‚îÇ                 ‚îÇ
   ‚îÇ ConvT1: 512‚Üí256 ‚îÇ  k=(3,1), s=(2,1) ‚Üí [B*T,256,11,1]
   ‚îÇ + BN + ReLU     ‚îÇ
   ‚îÇ                 ‚îÇ
   ‚îÇ ConvT2: 256‚Üí128 ‚îÇ  k=(3,1), s=(2,1) ‚Üí [B*T,128,22,1]
   ‚îÇ + BN + ReLU     ‚îÇ
   ‚îÇ                 ‚îÇ
   ‚îÇ ConvT3: 128‚Üí64  ‚îÇ  k=(3,1), s=(2,1) ‚Üí [B*T,64,44,1]
   ‚îÇ + BN + ReLU     ‚îÇ
   ‚îÇ                 ‚îÇ
   ‚îÇ ConvT4: 64‚Üí32   ‚îÇ  k=(3,1), s=(3,1) ‚Üí [B*T,32,132,1]
   ‚îÇ + BN + ReLU     ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ WIDTH RESTORER  ‚îÇ  Conv2d(32‚Üí160, k=(1,1)) + PixelShuffle
   ‚îÇ  [B*T,32,132,5] ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ FINAL PROJECTION‚îÇ  Conv2d(32‚Üí3, k=(1,1))
   ‚îÇ  [B*T,3,132,5]  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
OUTPUT: [B, T, 3, 132, 5]  ‚Üê‚îÄ‚îÄ Reconstructed input
```

---

## Dimension Flow Analysis

### Forward Pass Dimensions

| **Layer** | **Operation** | **Input Shape** | **Output Shape** | **Reduction Factor** |
|-----------|---------------|-----------------|------------------|---------------------|
| **Input** | - | `[B, T, 3, 132, 5]` | `[B, T, 3, 132, 5]` | 1.0√ó |
| **Reshape** | View | `[B, T, 3, 132, 5]` | `[B*T, 3, 132, 5]` | 1.0√ó |
| **Width Reducer** | Conv2d(3‚Üí32, k=(1,5)) | `[B*T, 3, 132, 5]` | `[B*T, 32, 132, 1]` | **5.0√ó** (width) |
| **Encoder Conv1** | Conv2d(32‚Üí64, k=(3,1), s=(3,1)) | `[B*T, 32, 132, 1]` | `[B*T, 64, 44, 1]` | **3.0√ó** (height) |
| **Encoder Conv2** | Conv2d(64‚Üí128, k=(3,1), s=(2,1)) | `[B*T, 64, 44, 1]` | `[B*T, 128, 22, 1]` | **2.0√ó** (height) |
| **Encoder Conv3** | Conv2d(128‚Üí256, k=(3,1), s=(2,1)) | `[B*T, 128, 22, 1]` | `[B*T, 256, 11, 1]` | **2.0√ó** (height) |
| **Encoder Conv4** | Conv2d(256‚Üí512, k=(3,1), s=(2,1)) | `[B*T, 256, 11, 1]` | `[B*T, 512, 5, 1]` | **2.2√ó** (height) |
| **AdaptiveAvgPool** | Global pooling | `[B*T, 512, 5, 1]` | `[B*T, 512, 1, 1]` | **5.0√ó** (height) |
| **Flatten** | Reshape | `[B*T, 512, 1, 1]` | `[B*T, 512]` | 1.0√ó |
| **Linear** | Linear(512‚Üí256) | `[B*T, 512]` | `[B*T, 256]` | **2.0√ó** |
| **Embedding** | Reshape | `[B*T, 256]` | `[B, T, 256]` | 1.0√ó |

### Compression Metrics

| **Stage** | **Elements** | **Memory (MB)** | **Compression Ratio** |
|-----------|--------------|-----------------|----------------------|
| **Input** | `B√óT√ó3√ó132√ó5 = 1,980√óB√óT` | `7.56√óB√óT` | **1.0√ó** (baseline) |
| **After Width Reduction** | `B√óT√ó32√ó132√ó1 = 4,224√óB√óT` | `16.1√óB√óT` | **0.47√ó** ‚ÜóÔ∏è |
| **After Conv1** | `B√óT√ó64√ó44√ó1 = 2,816√óB√óT` | `10.8√óB√óT` | **0.70√ó** ‚ÜóÔ∏è |
| **After Conv2** | `B√óT√ó128√ó22√ó1 = 2,816√óB√óT` | `10.8√óB√óT` | **0.70√ó** ‚ÜóÔ∏è |
| **After Conv3** | `B√óT√ó256√ó11√ó1 = 2,816√óB√óT` | `10.8√óB√óT` | **0.70√ó** ‚ÜóÔ∏è |
| **After Conv4** | `B√óT√ó512√ó5√ó1 = 2,560√óB√óT` | `9.77√óB√óT` | **0.77√ó** ‚ÜóÔ∏è |
| **Embedding** | `B√óT√ó256 = 256√óB√óT` | `1.0√óB√óT` | **7.73√ó** ‚ÜóÔ∏è‚ÜóÔ∏è |

> **Overall Compression**: **7.73√ó** reduction from input to embedding

---

## Layer-by-Layer Analysis

### 1. Width Reduction Stage
```python
self.width_reducer = nn.Sequential(
    nn.Conv2d(3, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 0)),
    nn.BatchNorm2d(32),
    nn.ReLU(inplace=True)
)
```

**Purpose**: Reduces width dimension from 5 to 1 while expanding channels
- **Input**: `[B*T, 3, 132, 5]` - 3-channel ultrasound frames
- **Output**: `[B*T, 32, 132, 1]` - 32-channel reduced-width features
- **Key Insight**: Collapses spatial width early, forcing model to encode width information in channel dimension

### 2. Encoder Backbone
```python
# Progressive channel expansion with spatial reduction
Conv1: 32‚Üí64  channels, 132‚Üí44  height (stride=3)
Conv2: 64‚Üí128 channels, 44‚Üí22   height (stride=2)  
Conv3: 128‚Üí256 channels, 22‚Üí11  height (stride=2)
Conv4: 256‚Üí512 channels, 11‚Üí5   height (stride=2)
```

**Design Pattern**: Each layer doubles channels while halving spatial resolution
- **Regularization**: Dropout2d(0.1) on first two layers prevents overfitting
- **Normalization**: BatchNorm2d for training stability
- **Activation**: ReLU for non-linearity

### 3. Bottleneck Compression
```python
nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling
nn.Flatten()                  # Reshape to vector
nn.Linear(512, embedding_dim) # Final compression
nn.LayerNorm(embedding_dim)   # Normalize embeddings
```

**Critical Stage**: Maximum compression point
- **Global Pooling**: Removes all spatial dependencies
- **Linear Projection**: Maps to desired embedding dimension
- **LayerNorm**: Ensures stable embedding distribution

### 4. Decoder Architecture
**Symmetric Reconstruction**: Mirrors encoder structure in reverse
- **Linear Expansion**: `embedding_dim ‚Üí 512*5` 
- **Spatial Restoration**: Progressive ConvTranspose2d layers
- **Channel Reduction**: `512‚Üí256‚Üí128‚Üí64‚Üí32` channels

### 5. Width Restoration
```python
self.width_restorer = nn.Sequential(
    nn.Conv2d(32, 32 * 5, kernel_size=(1, 1)),
    nn.PixelShuffle(1),  # Rearranges channels to spatial dimensions
)
```

**Width Recovery**: Expands from width=1 back to width=5
- **Channel Expansion**: `32 ‚Üí 160` channels
- **PixelShuffle**: Reorganizes channels into spatial width

---

## Mathematical Analysis

### Receptive Field Calculation

| **Layer** | **Kernel Size** | **Stride** | **Receptive Field** |
|-----------|-----------------|------------|---------------------|
| Width Reducer | (1, 5) | (1, 1) | **5** (width direction) |
| Conv1 | (3, 1) | (3, 1) | **5** (height direction) |
| Conv2 | (3, 1) | (2, 1) | **9** |
| Conv3 | (3, 1) | (2, 1) | **17** |
| Conv4 | (3, 1) | (2, 1) | **33** |

**Final Receptive Field**: **33 pixels** in height direction, capturing ~25% of input height (132 pixels)

### Parameter Count Analysis

| **Component** | **Parameters** | **Percentage** |
|---------------|----------------|----------------|
| **Width Reducer** | `3√ó32√ó1√ó5 + 32 = 512` | 0.1% |
| **Encoder Conv** | `~850k` | 15.2% |
| **Linear Layers** | `512√ó256 + 256√ó2560 = ~787k` | 14.1% |
| **Decoder Conv** | `~3.9M` | 70.6% |
| **Total** | **~5.54M parameters** | 100% |

---

## Design Critique & Analysis

### ‚úÖ **Strengths**

1. **Domain-Specific Design**
   - Width reduction targets ultrasound M-mode geometry
   - Progressive compression preserves hierarchical features
   - Temporal dimension handling for sequence data

2. **Training Stability**
   - BatchNorm on all conv layers
   - LayerNorm on embeddings
   - Gradient clipping support in trainer
   - Dropout for regularization

3. **Architectural Symmetry**
   - Encoder-decoder mirror structure
   - Systematic channel progression (32‚Üí64‚Üí128‚Üí256‚Üí512)
   - Proper dimension restoration

4. **Compression Efficiency**
   - **7.73√ó compression ratio** is substantial
   - Bottleneck preserves essential information
   - Global pooling removes spatial bias

### ‚ö†Ô∏è **Potential Issues**

1. **Information Bottleneck Risk**
   ```
   132√ó5 = 660 spatial positions ‚Üí 256 embedding dims
   Severe spatial compression (2.58√ó spatial reduction)
   ```

2. **Width Restoration Concern**
   ```python
   # PixelShuffle with factor=1 doesn't actually rearrange
   nn.PixelShuffle(1)  # This is effectively a no-op!
   ```

3. **Asymmetric Pooling**
   - Global average pooling loses all spatial information
   - No learned pooling or attention mechanism
   - May struggle with spatially-dependent features

4. **Fixed Architecture**
   - Hard-coded for `[132, 5]` input dimensions
   - No adaptive sizing for different ultrasound formats
   - Embedding dimension is fixed

### üîß **Suggested Improvements**

1. **Attention Mechanisms**
   ```python
   # Replace global pooling with attention
   self.spatial_attention = nn.MultiheadAttention(512, 8)
   ```

2. **Dynamic Width Restoration**
   ```python
   # Fix PixelShuffle implementation
   nn.PixelShuffle(5)  # Actually rearrange 32*5 ‚Üí 32 channels, 5√ó width
   ```

3. **Skip Connections**
   ```python
   # Add U-Net style connections
   skip_connections = {}  # Store encoder features
   # Concatenate in decoder for better reconstruction
   ```

4. **Learnable Pooling**
   ```python
   # Replace AdaptiveAvgPool with learnable alternative
   self.learned_pool = nn.Conv2d(512, 512, kernel_size=(5,1))
   ```

---

## Performance Characteristics

### Memory Usage (Estimated)
- **Training**: ~45MB per sample (forward + backward)
- **Inference**: ~22MB per sample
- **Peak Usage**: During backpropagation through decoder

### Computational Complexity
- **FLOPs**: ~2.1M per forward pass
- **Bottleneck**: ConvTranspose2d operations in decoder
- **Optimization**: Channel-wise operations dominate

### Training Considerations
- **Gradient Flow**: May suffer from vanishing gradients through deep decoder
- **Learning Rate**: Encoder vs decoder may need different rates
- **Batch Size**: Memory-limited due to temporal dimension

---

## Usage Examples

### Model Instantiation
```python
# Standard embedding size
model = CNNAutoencoder(embedding_dim=256)

# Larger embedding for more detail preservation
model = CNNAutoencoder(embedding_dim=512)

# Compact model for mobile/edge deployment
model = CNNAutoencoder(embedding_dim=128)
```

### Forward Pass
```python
# Input: [batch_size, seq_length, 3, 132, 5]
input_tensor = torch.randn(4, 10, 3, 132, 5)

# Forward pass
reconstruction, embedding = model(input_tensor)

# Shapes:
# reconstruction: [4, 10, 3, 132, 5] - same as input
# embedding: [4, 10, 256] - compressed representation
```

### Embedding Extraction
```python
# Extract only embeddings (encoder only)
with torch.no_grad():
    embeddings = model.encode(input_tensor)
    # Shape: [4, 10, 256]
```

---

## Related Documentation

- **Training**: See `training/trainers/trainer_ae.py` for training implementation
- **Data Loading**: See `data/loader.py` for input preprocessing
- **Configuration**: See `config/config.yaml` for hyperparameters
- **Evaluation**: See training scripts for reconstruction metrics

---

## Change Log

### Version 1.0 (2025-10-08)
- ‚úÖ Initial architecture analysis
- ‚úÖ Complete dimension flow mapping
- ‚úÖ Design critique and improvement suggestions
- ‚úÖ Mathematical analysis and parameter counts